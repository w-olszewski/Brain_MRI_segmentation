{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/w-olszewski/Brain_MRI_segmentation/blob/main/U_NET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBZ2fWv7PqFA",
        "outputId": "5b4baf0a-18f7-4479-d3c9-618b96187892"
      },
      "source": [
        "#imported libraries\n",
        "!curl -s -o colormap.txt https://raw.githubusercontent.com/thenineteen/Semiology-Visualisation-Tool/master/slicer/Resources/Color/BrainAnatomyLabelsV3_0.txt\n",
        "import imageio\n",
        "import multiprocessing\n",
        "import torch\n",
        "import torchio as tio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from imageio import imread\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import nibabel as nib\n",
        "import enum\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "from IPython import display\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkZyNCUhcx_T",
        "outputId": "5714c520-962f-4f0d-e07e-8bd2acd407e5"
      },
      "source": [
        "!pip install torchmetrics #.functional\n",
        "from torchmetrics.functional import dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HCM6kxMQDSm",
        "outputId": "baea28dd-2e62-421d-ebb0-92850f3bdf4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/MyDrive\n",
            "/content/MyDrive/MyDrive/Colab_Notebooks/UNET/MICCAI_MultiAtlasChallenge2012_corrected/Training\n",
            "/bin/bash: tree: command not found\n"
          ]
        }
      ],
      "source": [
        "# Accessing dataset\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/MyDrive')\n",
        "\n",
        "datafiles = os.path.abspath('/content/MyDrive/MyDrive/Colab_Notebooks/UNET/MICCAI_MultiAtlasChallenge2012_corrected/Training')\n",
        "\n",
        "datafiles_dir = Path(datafiles)\n",
        "\n",
        "%cd /content/MyDrive/MyDrive/Colab_Notebooks/UNET/MICCAI_MultiAtlasChallenge2012_corrected/Training\n",
        "\n",
        "!tree -d {datafiles_dir}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxRCaIEd17Do"
      },

      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "gssMjOxCNAQB",
        "outputId": "54941ea9-8674-499f-fc73-06d8a9cbe56d"
      },
      
      "source": [
        "# Training dataset\n",
        "\n",
        "images_dir = datafiles_dir / 'Images'\n",
        "masks_dir = datafiles_dir / 'SegmentMaps'\n",
        "image_paths = sorted(images_dir.glob('*.nii'))\n",
        "mask_paths = sorted(masks_dir.glob('*.nii'))\n",
        "assert len(image_paths) == len(mask_paths)\n",
        "\n",
        "subjects = []\n",
        "for (image_path, mask_path) in zip(image_paths, mask_paths):\n",
        "    subject = tio.Subject(\n",
        "        mri=tio.ScalarImage(image_path),\n",
        "        brain=tio.LabelMap(mask_path),\n",
        "    )\n",
        "\n",
        "    subjects.append(subject)\n",
        "dataset = tio.SubjectsDataset(subjects)\n",
        "print('Dataset size:', len(dataset), 'subjects')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtYAMZKO99w3",
        "outputId": "433f29af-0bd7-40d2-dd08-b79c09ea2144"
      },
   
      "source": [
        "# Testing dataset\n",
        "\n",
        "test_dir = datafiles_dir / '../Testing'\n",
        "\n",
        "test_paths = sorted(test_dir.glob('*.nii'))\n",
        "\n",
        "test_list = []\n",
        "for test_path in test_paths:\n",
        "    test_subject = tio.Subject(mri=tio.ScalarImage(test_path))\n",
        "\n",
        "    test_list.append(test_subject)\n",
        "testing_dataset = tio.SubjectsDataset(test_list)\n",
        "print('Validation dataset size:', len(testing_dataset), 'subjects')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "id": "Uhda_tikGtm6",
        "outputId": "1f2a995a-06c7-4c6c-e5e1-5848320fb545"
      },

      "source": [
        "one_subject = dataset[1]\n",
        "one_subject.plot()\n",
        "\n",
        "print(one_subject)\n",
        "print(one_subject.mri)\n",
        "print(one_subject.brain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "KQgzfC0g2euG",
        "outputId": "f8697c95-8f89-4388-ca99-9a126c28994b"
      },
    
      "source": [
        "# Training the histogram\n",
        "\n",
        "paths = image_paths\n",
        "compute_histograms = True\n",
        "\n",
        "def plot_histogram(axis, tensor, num_positions=100, label=None, alpha=0.05, color=None):\n",
        "    values = tensor.numpy().ravel()\n",
        "    kernel = stats.gaussian_kde(values)\n",
        "    positions = np.linspace(values.min(), values.max(), num=num_positions)\n",
        "    histogram = kernel(positions)\n",
        "    kwargs = dict(linewidth=1, color='black' if color is None else color, alpha=alpha)\n",
        "    if label is not None:\n",
        "        kwargs['label'] = label\n",
        "    axis.plot(positions, histogram, **kwargs)\n",
        "\n",
        "if compute_histograms:\n",
        "    fig, ax = plt.subplots(dpi=100)\n",
        "    for path in tqdm(paths):\n",
        "        tensor = tio.ScalarImage(path).data\n",
        "        color = 'red'\n",
        "        plot_histogram(ax, tensor, color=color)\n",
        "    ax.set_xlim(-100, 2000)\n",
        "    ax.set_ylim(0, 0.004);\n",
        "    ax.set_title('Original histograms of all samples')\n",
        "    ax.set_xlabel('Intensity')\n",
        "    ax.grid()\n",
        "    graph = None\n",
        "else:\n",
        "    graph = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "6725c59998db4b909d064d861234066c",
            "9493ade8e6ea4dab89b1f3106166b2d4",
            "b7bc23c54d484fbf849914774b886f16",
            "42ee46bbab514190819a7e0db750427a",
            "10d5f0d7e6b84fa5bd6a449a1268388d",
            "406a6ba4e3744c34ba2385c6d4e86c5f",
            "3014d37c5fe34f029cd3cf9fb7b8976a",
            "a164b95697ea471fb9229d6ded9c0c76",
            "aa795df42b774303be2e99e323343e57",
            "7f73352eefb641cba69592b348c84577",
            "d4e6743542d746a3a75895ac47723ae1"
          ]
        },
        "id": "N6gI9Tei2qeq",
        "outputId": "77c7d57a-edc9-445d-bd2c-8619d9aef624"
      },
   
      "source": [
        "histogram_landmarks_path = 'landmarks.npy'\n",
        "\n",
        "landmarks = tio.HistogramStandardization.train(\n",
        "    image_paths,\n",
        "    output_path=histogram_landmarks_path,\n",
        ")\n",
        "\n",
        "np.set_printoptions(suppress=True, precision=3)\n",
        "print('\\nTrained landmarks:', landmarks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "DuzdrXPC22pP",
        "outputId": "eb90166f-be64-4e2c-c6f5-622680b5a2ab"
      },
    
      "source": [
        "landmarks_dict = {'mri': landmarks}\n",
        "histogram_transform = tio.HistogramStandardization(landmarks_dict)\n",
        "\n",
        "if compute_histograms:\n",
        "    fig, ax = plt.subplots(dpi=100)\n",
        "    for i ,sample in enumerate(tqdm(dataset)):\n",
        "        standard = histogram_transform(sample)\n",
        "        tensor = standard.mri.data\n",
        "        path = str(sample.mri.path)\n",
        "        if 'HH' in path: color = 'red'\n",
        "        elif 'Guys' in path: color = 'green'\n",
        "        elif 'IOP' in path: color = 'blue'\n",
        "        plot_histogram(ax, tensor, color=color)\n",
        "    ax.set_xlim(0, 150)\n",
        "    ax.set_ylim(0, 0.02)\n",
        "    ax.set_title('Intensity values of all samples after histogram standardization')\n",
        "    ax.set_xlabel('Intensity')\n",
        "    ax.grid()\n",
        "    graph = None\n",
        "\n",
        "else:\n",
        "    graph = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "3HNEheMf3D4C",
        "outputId": "68718030-5655-4350-8d76-ad32fb9a1efa"
      },
    
      "source": [
        "znorm_transform = tio.ZNormalization(masking_method=tio.ZNormalization.mean)\n",
        "\n",
        "sample = dataset[0]\n",
        "transform = tio.Compose([histogram_transform, znorm_transform])\n",
        "znormed = transform(sample)\n",
        "\n",
        "fig, ax = plt.subplots(dpi=100)\n",
        "plot_histogram(ax, znormed.mri.data, label='Z-normed', alpha=1)\n",
        "ax.set_title('Intensity values of one sample after z-normalization')\n",
        "ax.set_xlabel('Intensity')\n",
        "ax.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97DWkgS6dt4c"
      },
      "outputs": [],
      "source": [
        "# Transformations\n",
        "\n",
        "training_transform = tio.Compose([\n",
        "    tio.ToCanonical(),\n",
        "    tio.Resample(4),\n",
        "    tio.CropOrPad((48, 56, 48)),\n",
        "    tio.RandomMotion(p=0.2),\n",
        "    tio.HistogramStandardization({'mri': landmarks}),\n",
        "    tio.RandomBiasField(p=0.3),\n",
        "    tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
        "    tio.RandomNoise(p=0.5),\n",
        "    tio.RandomFlip(),\n",
        "    tio.OneOf({\n",
        "        tio.RandomAffine(): 0.8,\n",
        "        tio.RandomElasticDeformation(): 0.2,\n",
        "    }),\n",
        "    tio.OneHot(),\n",
        "])\n",
        "\n",
        "validation_transform = tio.Compose([\n",
        "    tio.ToCanonical(),\n",
        "    tio.Resample(4),\n",
        "    tio.CropOrPad((48, 56, 48)),\n",
        "    tio.HistogramStandardization({'mri': landmarks}),\n",
        "    tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
        "    tio.OneHot(),\n",
        "])\n",
        "\n",
        "testing_transform = tio.Compose([\n",
        "    tio.HistogramStandardization({'mri': landmarks}),\n",
        "    tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
        "])\n",
        "\n",
        "training_split_ratio = 0.8\n",
        "\n",
        "num_subjects = len(dataset)\n",
        "num_training_subjects = int(training_split_ratio * num_subjects)\n",
        "num_validation_subjects = num_subjects - num_training_subjects\n",
        "\n",
        "num_split_subjects = num_training_subjects, num_validation_subjects\n",
        "training_subjects, validation_subjects = torch.utils.data.random_split(subjects, num_split_subjects)\n",
        "\n",
        "training_set = tio.SubjectsDataset(\n",
        "    training_subjects, transform=training_transform)\n",
        "\n",
        "validation_set = tio.SubjectsDataset(\n",
        "    validation_subjects, transform=validation_transform)\n",
        "\n",
        "testing_set = tio.SubjectsDataset(\n",
        "    testing_dataset, transform=testing_transform)\n",
        "\n",
        "\n",
        "print('Training set:', len(training_set), 'subjects')\n",
        "print('Validation set:', len(validation_set), 'subjects')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldVuWWFRf106"
      },
   
      "source": [
        "#@title\n",
        "# U-NET model\n",
        "# 4 convolution layers\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, input_channel=1, output_channel=208, num_filter=32):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder path\n",
        "\n",
        "        n = num_filter  # 32\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv3d(input_channel, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "\n",
        "        n = n * 2  # 64\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv3d(int(n / 2), n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "\n",
        "        n = n * 2  # 128\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv3d(int(n / 2), n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "\n",
        "        n = n * 2  # 256\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv3d(int(n / 2), n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.drop = nn.Dropout(p=0.5)\n",
        "\n",
        "       # Decoder path\n",
        "\n",
        "        n = int(n / 2)  # 128\n",
        "        self.up3 = nn.ConvTranspose3d(n * 2, n, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "        self.conv_up3 = nn.Sequential(\n",
        "            nn.Conv3d(n * 2, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        n = int(n / 2)  # 64\n",
        "        self.up2 = nn.ConvTranspose3d(n * 2, n, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "        self.conv_up2 = nn.Sequential(\n",
        "            nn.Conv3d(n * 2, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        n = int(n / 2)  # 32\n",
        "        self.up1 = nn.ConvTranspose3d(n * 2, n, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "        self.conv_up1 = nn.Sequential(\n",
        "            nn.Conv3d(n * 2, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(n),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.out = nn.Conv3d(n, output_channel, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        conv1_skip = x\n",
        "\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        conv2_skip = x\n",
        "\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        conv3_skip = x\n",
        "\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.up3(x)\n",
        "        x = self.conv_up3(torch.cat([conv3_skip, x], dim=1))\n",
        "\n",
        "        x = self.up2(x)\n",
        "        x = self.conv_up2(torch.cat([conv2_skip, x], dim=1))\n",
        "\n",
        "        x = self.up1(x)\n",
        "        x = self.conv_up1(torch.cat([conv1_skip, x], dim=1))\n",
        "\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "x0mgSLQwK5tr",
        "outputId": "98d6af4d-fb92-4c09-bb24-adc699aecc61"
      },
    
      "source": [
        "# Training units\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
        "CHANNELS_DIMENSION = 1\n",
        "SPATIAL_DIMENSIONS = 2, 3, 4\n",
        "num_classes = 208\n",
        "\n",
        "\n",
        "def prepare_batch(batch, device):\n",
        "    inputs = batch['mri'][tio.DATA].to(device)\n",
        "    targets = batch['brain'][tio.DATA].to(device)\n",
        "    return inputs, targets\n",
        "\n",
        "def get_model_and_optimizer(device):\n",
        "    model = UNet(input_channel=1, output_channel=num_classes, num_filter=32).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
        "    return model, optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0A5ojxptf9E3",
        "outputId": "39889091-7cf1-4da6-9c72-6c9dbe8f9ab5"
      },
      
      "source": [
        "training_instance = training_set[4]  # transform is applied inside SubjectsDataset\n",
        "training_instance.plot()\n",
        "print(training_instance.mri)\n",
        "print(training_instance.brain)\n",
        "\n",
        "validation_instance = validation_set[1]  # transform is applied inside SubjectsDataset\n",
        "validation_instance.plot()\n",
        "print(validation_instance.mri)\n",
        "print(validation_instance.brain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n1vJUmtDreo"
      },
   
      "source": [
        "training_batch_size = 32\n",
        "validation_batch_size = 2 * training_batch_size\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(\n",
        "    training_set,\n",
        "    batch_size=training_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        ")\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    validation_set,\n",
        "    batch_size=validation_batch_size,\n",
        "    num_workers=num_workers,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YN_ofzOMh9U6"
      },
     
      "source": [
        "# Patch-based training\n",
        "\n",
        "\n",
        "patch_size = 24\n",
        "samples_per_volume = 5\n",
        "max_queue_length = 300\n",
        "sampler = tio.data.UniformSampler(patch_size)\n",
        "\n",
        "patches_training_set = tio.Queue(\n",
        "    subjects_dataset=training_set,\n",
        "    max_length=max_queue_length,\n",
        "    samples_per_volume=samples_per_volume,\n",
        "    sampler=sampler,\n",
        "    num_workers=num_workers,\n",
        "    shuffle_subjects=True,\n",
        "    shuffle_patches=True,\n",
        ")\n",
        "\n",
        "patches_validation_set = tio.Queue(\n",
        "    subjects_dataset=validation_set,\n",
        "    max_length=max_queue_length,\n",
        "    samples_per_volume=samples_per_volume,\n",
        "    sampler=sampler,\n",
        "    num_workers=num_workers,\n",
        "    shuffle_subjects=False,\n",
        "    shuffle_patches=False,\n",
        ")\n",
        "\n",
        "training_loader_patches = torch.utils.data.DataLoader(\n",
        "    patches_training_set, batch_size=training_batch_size)\n",
        "\n",
        "validation_loader_patches = torch.utils.data.DataLoader(\n",
        "    patches_validation_set, batch_size=validation_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "N2jaY2KdiDH6",
        "outputId": "9a78c029-208a-4ec1-d260-f3628955f4ce"
      },
     
      "source": [
        "# Patch-based training\n",
        "\n",
        "one_batch = next(iter(training_loader_patches))\n",
        "k = int(patch_size // 4)\n",
        "batch_mri = one_batch['mri'][tio.DATA][..., k]\n",
        "batch_label = one_batch['brain'][tio.DATA][:, 207:, ..., k]\n",
        "slices = torch.cat((batch_mri, batch_label))\n",
        "image_path = 'batch_patches.png'\n",
        "torchvision.utils.save_image(\n",
        "    slices,\n",
        "    image_path,\n",
        "    nrow=training_batch_size//2,\n",
        "    normalize=True,\n",
        "    scale_each=True,\n",
        ")\n",
        "display.Image(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rfztXeXDpLQg"
      },
     
      "source": [
        "\n",
        "model_dir = 'saved_models'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "model = UNet().to(device)\n",
        "model.load_state_dict(torch.load(os.path.join(model_dir, 'model.pt'), map_location=device))\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training\n",
        "\n",
        "num_iter = 10000\n",
        "train_batch_size = 16\n",
        "eval_batch_size = 8\n",
        "start = time.time()\n",
        "for it in range(1, 1 + num_iter):\n",
        "\n",
        "    start_iter = time.time()\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, batch in enumerate(tqdm(training_loader_patches)):\n",
        "        images, labels = prepare_batch(batch, device)\n",
        "    images, labels = images.clone().detach().requires_grad_(True), labels.clone().detach().requires_grad_(True)\n",
        "    images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.float32)\n",
        "    logits = model(images)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(logits, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('--- Iteration {0}: Training loss = {1:.4f}, {2:.4f} s ---'.format(it, loss.item(), time.time() - start_iter))\n",
        "\n",
        "    # Evaluate\n",
        "\n",
        "    if it % 10 == 0:\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(tqdm(validation_loader_patches)):\n",
        "                images, labels = prepare_batch(batch, device)\n",
        "\n",
        "            images, labels = images.clone().detach(), labels.clone().detach()\n",
        "            images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.float32)\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            print('--- Iteration {0}: Test loss = {1:.4f} ---\\n'.format(it, loss.item()))\n",
        "\n",
        "    # Save\n",
        "\n",
        "    if it % 1000 == 0:\n",
        "        torch.save(model.state_dict(), os.path.join(model_dir, 'model_wholeimg_{0}.pt'.format(it+4000)))\n",
        "print('Training took {:.3f}s in total.'.format(time.time() - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg5h7PODw7rV"
      },
   
      "source": [
        "# Testing\n",
        "\n",
        "from model_dir import 'model.pt\n",
        "\n",
        "for batch_idx, batch in enumerate(tqdm(validation_loader)):\n",
        "    images, labels = prepare_batch(batch, device)\n",
        "images, labels = images.clone().detach(), labels.clone().detach()\n",
        "images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.float32)\n",
        "model.eval()\n",
        "logits = model(images)\n",
        "prob = F.softmax(logits, dim=1)\n",
        "seg = torch.argmax(prob, dim=0)\n",
        "accuracy = (seg == labels).cpu().numpy().mean() * 100\n",
        "dice_score = dice(prob, labels.int(), average='micro').to(device)\n",
        "\n",
        "print(accuracy)\n",
        "print(dice_score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "authorship_tag": "ABX9TyM1amZtmq+oAGrdX1A45iUC",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10d5f0d7e6b84fa5bd6a449a1268388d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3014d37c5fe34f029cd3cf9fb7b8976a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "406a6ba4e3744c34ba2385c6d4e86c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42ee46bbab514190819a7e0db750427a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f73352eefb641cba69592b348c84577",
            "placeholder": "​",
            "style": "IPY_MODEL_d4e6743542d746a3a75895ac47723ae1",
            "value": " 15/15 [00:10&lt;00:00,  1.42it/s]"
          }
        },
        "6725c59998db4b909d064d861234066c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9493ade8e6ea4dab89b1f3106166b2d4",
              "IPY_MODEL_b7bc23c54d484fbf849914774b886f16",
              "IPY_MODEL_42ee46bbab514190819a7e0db750427a"
            ],
            "layout": "IPY_MODEL_10d5f0d7e6b84fa5bd6a449a1268388d"
          }
        },
        "7f73352eefb641cba69592b348c84577": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9493ade8e6ea4dab89b1f3106166b2d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_406a6ba4e3744c34ba2385c6d4e86c5f",
            "placeholder": "​",
            "style": "IPY_MODEL_3014d37c5fe34f029cd3cf9fb7b8976a",
            "value": "100%"
          }
        },
        "a164b95697ea471fb9229d6ded9c0c76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa795df42b774303be2e99e323343e57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7bc23c54d484fbf849914774b886f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a164b95697ea471fb9229d6ded9c0c76",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa795df42b774303be2e99e323343e57",
            "value": 15
          }
        },
        "d4e6743542d746a3a75895ac47723ae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
